"""
learning_engine.py - Há»‡ thá»‘ng tá»± há»c tá»« feedback ngÆ°á»i dÃ¹ng

TÃ­nh nÄƒng:
- LÆ°u trá»¯ cÃ¡c cÃ¢u há»i/cÃ¢u tráº£ lá»i Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng Ä‘Ã¡nh giÃ¡ tá»‘t
- Ghi nháº­n feedback (tá»‘t/xáº¥u) vÃ  tá»« khÃ³a tÆ°Æ¡ng tá»±
- Tá»•ng há»£p patterns Ä‘á»ƒ cáº£i thiá»‡n cÃ¢u tráº£ lá»i tÆ°Æ¡ng lai
- Pattern matching Ä‘á»ƒ detect cÃ¢u há»i tÆ°Æ¡ng tá»±
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict, Counter
import re


class LearningEngine:
    """Quáº£n lÃ½ há»c táº­p tá»« feedback ngÆ°á»i dÃ¹ng"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.learning_file = os.path.join(data_dir, "learned_interactions.json")
        self.patterns_file = os.path.join(data_dir, "learned_patterns.json")
        self.synonyms_file = os.path.join(data_dir, "learned_synonyms.json")
        self.feedback_file = os.path.join(data_dir, "feedback_stats.json")
        
        os.makedirs(data_dir, exist_ok=True)
        
        # Táº£i dá»¯ liá»‡u hiá»‡n cÃ³
        self.interactions = self._load_json(self.learning_file, [])
        self.patterns = self._load_json(self.patterns_file, {})
        self.synonyms = self._load_json(self.synonyms_file, {})
        self.feedback_stats = self._load_json(self.feedback_file, {
            "total_interactions": 0,
            "positive_feedback": 0,
            "negative_feedback": 0,
            "avg_rating": 0.0,
            "most_asked": []
        })
    
    def _load_json(self, filepath: str, default=None):
        """Load JSON file or return default"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Error loading {filepath}: {e}")
        return default if default is not None else {}
    
    def _save_json(self, filepath: str, data):
        """Save data to JSON file"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ Error saving {filepath}: {e}")
    
    def record_interaction(self, query: str, answer: str, sources: List[str], 
                          user_id: str = "anonymous", metadata: Dict = None) -> str:
        """Ghi nháº­n má»™t tÆ°Æ¡ng tÃ¡c (cÃ¢u há»i + cÃ¢u tráº£ lá»i)"""
        interaction = {
            "id": self._generate_id(),
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "answer": answer,
            "sources": sources,
            "user_id": user_id,
            "rating": 0,
            "feedback": None,
            "metadata": metadata or {},
            "query_normalized": self._normalize_query(query),
            "query_tokens": self._tokenize(query)
        }
        
        self.interactions.append(interaction)
        self._save_json(self.learning_file, self.interactions)
        
        # Update stats
        self.feedback_stats["total_interactions"] += 1
        self._save_json(self.feedback_file, self.feedback_stats)
        
        return interaction["id"]
    
    def submit_feedback(self, interaction_id: str, rating: int, feedback_text: str = "", 
                       is_helpful: bool = None):
        """NgÆ°á»i dÃ¹ng feedback cÃ¢u tráº£ lá»i (rating 1-5, true/false)"""
        for inter in self.interactions:
            if inter["id"] == interaction_id:
                inter["rating"] = rating
                inter["feedback"] = feedback_text
                inter["feedback_timestamp"] = datetime.now().isoformat()
                
                # Update stats
                if rating >= 4:
                    self.feedback_stats["positive_feedback"] += 1
                elif rating <= 2:
                    self.feedback_stats["negative_feedback"] += 1
                
                # Update average rating
                ratings = [i.get("rating", 0) for i in self.interactions if i.get("rating", 0) > 0]
                if ratings:
                    self.feedback_stats["avg_rating"] = sum(ratings) / len(ratings)
                
                # Extract learned patterns from positive feedback
                if rating >= 4:
                    self._learn_from_positive(inter)
                
                self._save_json(self.learning_file, self.interactions)
                self._save_json(self.feedback_file, self.feedback_stats)
                break
    
    def _learn_from_positive(self, interaction: Dict):
        """Há»c tá»« nhá»¯ng feedback tÃ­ch cá»±c"""
        query = interaction["query"]
        answer = interaction["answer"]
        tokens = interaction["query_tokens"]
        
        # TÄƒng táº§n suáº¥t cá»§a cÃ¡c tá»« khÃ³a
        for token in tokens:
            if token not in self.patterns:
                self.patterns[token] = {
                    "frequency": 0,
                    "answers": [],
                    "success_rate": 0.0
                }
            self.patterns[token]["frequency"] += 1
            
            # LÆ°u trá»¯ pattern cá»§a cÃ¢u tráº£ lá»i
            if answer not in self.patterns[token]["answers"]:
                self.patterns[token]["answers"].append(answer[:500])  # Limit answer length
        
        self._save_json(self.patterns_file, self.patterns)
    
    def find_similar_learned_answers(self, query: str, top_k: int = 3) -> List[Dict]:
        """TÃ¬m cÃ¡c cÃ¢u tráº£ lá»i tÆ°Æ¡ng tá»± tá»« nhá»¯ng cÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c há»c"""
        query_tokens = set(self._tokenize(query))
        
        similar = []
        for inter in self.interactions:
            if inter.get("rating", 0) >= 4:  # Chá»‰ láº¥y nhá»¯ng cÃ¢u tráº£ lá»i Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ tá»‘t
                inter_tokens = set(inter.get("query_tokens", []))
                
                # TÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± Jaccard
                if query_tokens and inter_tokens:
                    intersection = len(query_tokens & inter_tokens)
                    union = len(query_tokens | inter_tokens)
                    similarity = intersection / union if union > 0 else 0
                    
                    if similarity > 0.3:  # Threshold
                        similar.append({
                            "similarity": similarity,
                            "query": inter["query"],
                            "answer": inter["answer"],
                            "rating": inter.get("rating", 0)
                        })
        
        # Sort by similarity
        similar.sort(key=lambda x: x["similarity"], reverse=True)
        return similar[:top_k]
    
    def get_synonyms(self, word: str) -> List[str]:
        """Láº¥y tá»« Ä‘á»“ng nghÄ©a Ä‘Ã£ há»c hoáº·c máº·c Ä‘á»‹nh"""
        vietnamese_synonyms = {
            "Ä‘áº¥t": ["máº£nh Ä‘áº¥t", "thá»­a Ä‘áº¥t", "báº¥t Ä‘á»™ng sáº£n", "tÃ i sáº£n Ä‘áº¥t Ä‘ai"],
            "luáº­t": ["phÃ¡p luáº­t", "quy Ä‘á»‹nh", "Ä‘iá»u luáº­t", "bá»™ luáº­t"],
            "quyá»n": ["chá»§ quyá»n", "quyá»n háº¡n", "tÃ i quyá»n", "yÃªu cáº§u"],
            "nghÄ©a vá»¥": ["bá»•n pháº­n", "trÃ¡ch nhiá»‡m", "dá»¥ng vá»¥"],
            "vi pháº¡m": ["pháº¡m phÃ¡p", "infringement", "lá»—i pháº¡m", "vi phÃ¡p"],
            "xá»­ pháº¡t": ["pháº¡t tiá»n", "hÃ¬nh pháº¡t", "xá»­ lÃ½", "cÃ³ háº­u quáº£"],
            "mua": ["sá»Ÿ há»¯u", "sá»Ÿ lÄ©nh", "chiáº¿m há»¯u", "chuyÃªn há»¯u"],
            "bÃ¡n": ["chuyá»ƒn nhÆ°á»£ng", "phÃ¡t hÃ nh", "tiÃªu thá»¥"],
            "cho thuÃª": ["khoÃ¡n", "thuÃª bao", "cho sá»­ dá»¥ng"],
            "xÃ¢y dá»±ng": ["khai thÃ¡c", "phÃ¡t triá»ƒn", "cÃ´ng trÃ¬nh"],
        }
        
        if word in vietnamese_synonyms:
            return vietnamese_synonyms[word]
        
        # TÃ¬m tá»« Ä‘á»“ng nghÄ©a Ä‘Ã£ há»c
        if word in self.synonyms:
            return self.synonyms[word]
        
        return []
    
    def record_synonym_pair(self, word1: str, word2: str):
        """Ghi nháº­n cáº·p tá»« Ä‘á»“ng nghÄ©a"""
        if word1 not in self.synonyms:
            self.synonyms[word1] = []
        if word2 not in self.synonyms[word1]:
            self.synonyms[word1].append(word2)
        
        if word2 not in self.synonyms:
            self.synonyms[word2] = []
        if word1 not in self.synonyms[word2]:
            self.synonyms[word2].append(word1)
        
        self._save_json(self.synonyms_file, self.synonyms)
    
    def get_learning_stats(self) -> Dict:
        """Láº¥y thá»‘ng kÃª há»c táº­p"""
        return {
            **self.feedback_stats,
            "total_patterns_learned": len(self.patterns),
            "total_synonym_pairs": len(self.synonyms),
            "interactions_with_feedback": sum(1 for i in self.interactions if i.get("rating", 0) > 0)
        }
    
    def suggest_improvements(self, query: str, current_answer: str) -> List[str]:
        """Gá»£i Ã½ cÃ¡ch cáº£i thiá»‡n cÃ¢u tráº£ lá»i dá»±a trÃªn nhá»¯ng tÆ°Æ¡ng tÃ¡c tÆ°Æ¡ng tá»±"""
        suggestions = []
        
        # TÃ¬m cÃ¢u há»i tÆ°Æ¡ng tá»±
        similar = self.find_similar_learned_answers(query, top_k=5)
        
        if similar:
            suggestions.append(f"ðŸ’¡ TÃ¬m tháº¥y {len(similar)} cÃ¢u há»i tÆ°Æ¡ng tá»±")
            for idx, sim in enumerate(similar[:2], 1):
                if sim["similarity"] > 0.5:
                    suggestions.append(f"  {idx}. CÃ¢u há»i tÆ°Æ¡ng tá»±: '{sim['query']}' "
                                     f"(Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ {sim['rating']}/5)")
        
        # Gá»£i Ã½ dá»±a trÃªn patterns
        query_tokens = self._tokenize(query)
        related_patterns = []
        
        for token in query_tokens:
            if token in self.patterns:
                freq = self.patterns[token]["frequency"]
                if freq > 2:
                    related_patterns.append((token, freq))
        
        if related_patterns:
            related_patterns.sort(key=lambda x: x[1], reverse=True)
            suggestions.append(f"ðŸ”‘ CÃ¡c tá»« khÃ³a chÃ­nh: {', '.join([p[0] for p in related_patterns[:3]])}")
        
        return suggestions
    
    def _normalize_query(self, query: str) -> str:
        """Chuáº©n hÃ³a query: viáº¿t thÆ°á»ng, bá» dáº¥u"""
        query = query.lower()
        # Bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t nhÆ°ng giá»¯ tá»«
        query = re.sub(r'[^\w\sÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]', '', query)
        return query.strip()
    
    def _tokenize(self, text: str) -> List[str]:
        """TÃ¡ch tá»« tá»« text"""
        text = self._normalize_query(text)
        # Bá» cÃ¡c stop words phá»• biáº¿n
        stop_words = {'cÃ¡c', 'vÃ ', 'hay', 'lÃ ', 'Ä‘Æ°á»£c', 'Ä‘á»ƒ', 'trong', 'á»Ÿ', 'vá»', 'tá»«', 'vá»›i', 
                     'nhÆ°', 'cÃ¡i', 'cÃ¡i gÃ¬', 'gÃ¬', 'ai', 'khÃ´ng', 'cÃ³', 'báº¡n', 'tÃ´i', 'mÃ¬nh'}
        
        tokens = text.split()
        return [t for t in tokens if t not in stop_words and len(t) > 2]
    
    def _generate_id(self) -> str:
        """Táº¡o ID unique cho interaction"""
        import uuid
        return str(uuid.uuid4())[:8]
    
    def get_top_questions(self, limit: int = 10) -> List[Dict]:
        """Láº¥y nhá»¯ng cÃ¢u há»i Ä‘Æ°á»£c há»i nhiá»u nháº¥t"""
        question_counts = Counter()
        for inter in self.interactions:
            normalized = inter.get("query_normalized", "")
            if normalized:
                question_counts[normalized] += 1
        
        top = question_counts.most_common(limit)
        
        result = []
        for normalized_q, count in top:
            # TÃ¬m cÃ¢u há»i gá»‘c tÆ°Æ¡ng á»©ng
            for inter in self.interactions:
                if inter.get("query_normalized") == normalized_q:
                    result.append({
                        "question": inter["query"],
                        "count": count,
                        "avg_rating": inter.get("rating", 0)
                    })
                    break
        
        return result
    
    def export_learned_data(self, output_dir: str = "data/learned_exports"):
        """Export dá»¯ liá»‡u há»c Ä‘Æ°á»£c Ä‘á»ƒ phÃ¢n tÃ­ch"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Export all interactions with high rating
        high_quality = [i for i in self.interactions if i.get("rating", 0) >= 4]
        with open(os.path.join(output_dir, "high_quality_qa.json"), 'w', encoding='utf-8') as f:
            json.dump(high_quality, f, ensure_ascii=False, indent=2)
        
        # Export patterns
        with open(os.path.join(output_dir, "patterns.json"), 'w', encoding='utf-8') as f:
            json.dump(self.patterns, f, ensure_ascii=False, indent=2)
        
        # Export stats
        stats = self.get_learning_stats()
        with open(os.path.join(output_dir, "stats.json"), 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Exported learned data to {output_dir}")


# Global instance
_learning_engine = None

def get_learning_engine() -> LearningEngine:
    """Get hoáº·c táº¡o global learning engine instance"""
    global _learning_engine
    if _learning_engine is None:
        _learning_engine = LearningEngine()
    return _learning_engine
