# file: backend/bot.py
"""
Advanced retrieval-based chatbot with reasoning and scenario analysis.
- Smart context extraction and summarization
- Multi-intent understanding with confidence scoring
- Natural language composition similar to ChatGPT/Copilot
- Template-free dynamic answer generation
- Reasoning engine: comparing law provisions, practical scenarios, recommendations
- Calculation engine: penalties, fees, time limits from regulations
- Learning engine: learns from user feedback
- Sentiment analysis: understands user emotion & adjusts tone
- NLG engine: generates natural-sounding responses with variations
"""
from typing import List, Dict, Tuple, Optional
from backend.search import retrieve
from chatbot.learning_engine import get_learning_engine
from chatbot.sentiment_analyzer import get_sentiment_analyzer
from chatbot.conversation_manager import get_conversation_manager
from chatbot.nlg_engine import get_nlg_engine
import re
import random
from collections import defaultdict
import json


# Dynamic greeting system
GREETING_RESPONSES = [
    "Ch√†o b·∫°n! üëã M√¨nh l√† tr·ª£ l√Ω ph√°p lu·∫≠t ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi AI. H·ªèi t√¥i b·∫•t k·ª≥ ƒëi·ªÅu g√¨ v·ªÅ lu·∫≠t ƒë·∫•t ƒëai v√† m√¨nh s·∫Ω gi√∫p b·∫°n.",
    "Xin ch√†o! üòä T√¥i ·ªü ƒë√¢y ƒë·ªÉ gi·∫£i ƒë√°p m·ªçi th·∫Øc m·∫Øc c·ªßa b·∫°n v·ªÅ ph√°p lu·∫≠t m·ªôt c√°ch r√µ r√†ng v√† d·ªÖ hi·ªÉu.",
    "Ch√†o! T√¥i l√† ChatBot ph√°p lu·∫≠t c·ªßa b·∫°n. H√£y c·ª© h·ªèi, m√¨nh s·∫Ω c·ªë g·∫Øng tr·∫£ l·ªùi t·ªët nh·∫•t.",
    "Hola! üëã C√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ lu·∫≠t ƒë·∫•t ƒëai h√¥m nay?",
]

# Modern response templates
NO_RESULT_TEMPLATES = [
    "Xin l·ªói, m√¨nh kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y trong c∆° s·ªü d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t l·∫°i ho·∫∑c h·ªèi v·ªÅ m·ªôt kh√≠a c·∫°nh kh√°c kh√¥ng?",
    "T√¥i ch∆∞a c√≥ d·ªØ li·ªáu chi ti·∫øt v·ªÅ ƒëi·ªÅu n√†y. H√£y th·ª≠ h·ªèi l·∫°i v·ªõi t·ª´ kh√≥a kh√°c ho·∫∑c m·ªôt c√¢u h·ªèi li√™n quan.",
    "C√¢u h·ªèi n√†y c√≥ v·∫ª n·∫±m ngo√†i ph·∫°m vi c·ªßa t√¥i. Nh∆∞ng m√¨nh c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi c√°c c√¢u h·ªèi kh√°c li√™n quan ƒë·∫øn lu·∫≠t ƒë·∫•t ƒëai.",
]

# Confidence-based response modifiers
CONFIDENCE_PREFIXES = {
    'very_high': "ƒê√¢y l√† th√¥ng tin t·ª´ ph√°p lu·∫≠t ch√≠nh th·ª©c:",
    'high': "D·ª±a tr√™n c√°c t√†i li·ªáu ph√°p lu·∫≠t:",
    'medium': "Theo th√¥ng tin t√¨m ƒë∆∞·ª£c (c·∫ßn ki·ªÉm tra th√™m):",
    'low': "‚ö†Ô∏è Th√¥ng tin li√™n quan nh∆∞ng c·∫ßn x√°c nh·∫≠n t·ª´ c∆° quan ch·ª©c nƒÉng:",
}

CONFIDENCE_SUFFIXES = {
    'very_high': "Th√¥ng tin n√†y ƒë∆∞·ª£c tr√≠ch t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t ch√≠nh th·ª©c.",
    'high': "B·∫°n n√™n x√°c nh·∫≠n th√™m v·ªõi c∆° quan li√™n quan ƒë·ªÉ ch·∫Øc ch·∫Øn.",
    'medium': "‚ö†Ô∏è B·∫°n n√™n tham kh·∫£o th√™m c√°c ngu·ªìn kh√°c ho·∫∑c li√™n h·ªá c∆° quan ph√°p lu·∫≠t.",
    'low': "‚ö†Ô∏è B·∫°n N√äN li√™n h·ªá v·ªõi c∆° quan ph√°p lu·∫≠t ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c. Th√¥ng tin n√†y c√≥ ƒë·ªô tin c·∫≠y th·∫•p.",
}


# ============ REASONING & SCENARIO ANALYSIS ENGINE ============

def detect_scenario_query(query: str) -> bool:
    """Detect if query is about a practical scenario (not generic law question).
    
    Returns True if query describes a personal situation or asks for practical advice,
    False if it's a generic law question.
    """
    scenario_keywords = [
        r'\bt√¥i (c√≥|mu·ªën|c·∫ßn|s·∫Ω|ƒëang)\b',  # I am doing something
        r'\bm√¨nh (c√≥|mu·ªën|c·∫ßn|s·∫Ω|ƒëang)\b',  # We are doing something
        r'\bn·∫øu\b',  # if (conditional scenario)
        r'\btr∆∞·ªùng h·ª£p\b',  # case/scenario
        r'\bt√¨nh hu·ªëng\b',  # situation
        r'\bn√™n l√†m g√¨\b|\bph·∫£i l√†m g√¨\b|\bn√™n nh∆∞ th·∫ø n√†o\b',  # what should I do
        r'\bc√≥ ƒë∆∞·ª£c kh√¥ng\b|\bƒë∆∞·ª£c kh√¥ng\b|\bc√≥ th·ªÉ kh√¥ng\b',  # is it allowed
    ]
    q_lower = query.lower()
    
    # Check if this is a scenario query
    is_scenario = any(re.search(pattern, q_lower) for pattern in scenario_keywords)
    
    # But exclude generic "what is X" questions even if they match
    if is_scenario:
        # If it's purely asking "X l√† g√¨?" (what is X?), it's not a scenario
        if re.match(r'^[^?]*l√† g√¨\?$', q_lower):
            return False
        # If asking about definition/concept, not scenario
        if any(w in q_lower for w in ['kh√°i ni·ªám', 'ƒë·ªãnh nghƒ©a', '√Ω nghƒ©a', 'ƒë∆∞·ª£c hi·ªÉu l√†']):
            return False
    
    return is_scenario


def extract_scenario_context(query: str) -> Dict:
    """Extract key information from scenario query."""
    context = {
        'query': query,
        'subject': None,  # ng∆∞·ªùi d√πng th·ª±c hi·ªán h√†nh ƒë·ªông
        'action': None,  # h√†nh ƒë·ªông: mua, b√°n, x√¢y d·ª±ng, etc.
        'object': None,  # ƒë·ªëi t∆∞·ª£ng: ƒë·∫•t, nh√†, quy·ªÅn s·ª≠ d·ª•ng, etc.
        'conditions': [],  # ƒëi·ªÅu ki·ªán: c√≥ l·ª£i nhu·∫≠n, trong th√†nh ph·ªë, etc.
    }
    
    q_lower = query.lower()
    
    # Detect action type
    actions = {
        'mua|s·ªü h·ªØu': 'mua', 'b√°n|chuy·ªÉn nh∆∞·ª£ng|chuy·ªÉn': 'b√°n',
        'cho thu√™|cho s·ª≠ d·ª•ng': 'cho_thu√™', 'x√¢y d·ª±ng|x√¢y|khai th√°c': 'x√¢y_d·ª±ng',
        'di ch√∫c|th·ª´a k·∫ø': 'th·ª´a_k·∫ø', 'c·∫•p|c·∫•p ph√©p': 'xin_ph√©p'
    }
    
    for pattern, action_type in actions.items():
        if re.search(pattern, q_lower):
            context['action'] = action_type
            break
    
    # Detect object type
    objects = {
        r'ƒë·∫•t n√¥ng nghi·ªáp': 'ƒë·∫•t_n√¥ng_nghi·ªáp',
        r'ƒë·∫•t phi n√¥ng nghi·ªáp|th·ªï c∆∞|·ªü': 'ƒë·∫•t_c·ª•_th·ªÉ',
        r'ƒë·∫•t\b': 'ƒë·∫•t',
        r'nh√†\b|nh√† ·ªü|nh√† c·ª≠a': 'nh√†',
        r'quy·ªÅn s·ª≠ d·ª•ng': 'quy·ªÅn',
        r'b·∫•t ƒë·ªông s·∫£n': 'b·∫•t_ƒë·ªông_s·∫£n'
    }
    
    for pattern, obj_type in objects.items():
        if re.search(pattern, q_lower):
            context['object'] = obj_type
            break
    
    # Extract location or special conditions
    locations = re.findall(r'(th√†nh ph·ªë|qu·∫≠n|huy·ªán|t·ªânh|th√¥n|x√£|trong n∆∞·ªõc ngo√†i)', q_lower)
    if locations:
        context['conditions'].append(f"ƒê·ªãa ƒëi·ªÉm: {locations[0]}")
    
    # Check for business/profit intent
    if re.search(r'kinh doanh|l·ª£i nhu·∫≠n|thu nh·∫≠p|doanh nghi·ªáp', q_lower):
        context['conditions'].append('M·ª•c ƒë√≠ch kinh doanh')
        context['requires_permit'] = True  # <- Th√™m flag b·∫Øt gi·∫•y ph√©p kinh doanh
    
    return context


def analyze_scenario(query: str, context: Dict, hits: List[Dict]) -> str:
    """Analyze practical scenario based on law provisions and reasoning."""
    if not hits:
        return ""
    
    # Combine all relevant law text
    all_texts = []
    for h in hits:
        noi_dung = h.get('noi_dung', '')
        if isinstance(noi_dung, list):
            for article in noi_dung:
                if isinstance(article, dict):
                    text = article.get('noi_dung', '')
                    all_texts.append(text)
        else:
            all_texts.append(noi_dung)
    
    combined_text = ' '.join(all_texts)
    
    # Build reasoning response
    reasoning_parts = []
    reasoning_parts.append("### üìã Ph√¢n t√≠ch t√¨nh hu·ªëng c·ªßa b·∫°n:\n")
    
    # Analyze based on action type
    action = context.get('action')
    obj = context.get('object')
    
    if action == 'mua' or action == 's·ªü h·ªØu':
        reasoning_parts.append("**V·ªÅ vi·ªác mua/s·ªü h·ªØu:**")
        if obj in ('ƒë·∫•t_n√¥ng_nghi·ªáp', 'ƒë·∫•t_c·ª•_th·ªÉ') or 'n√¥ng nghi·ªáp' in query.lower():
            if 'n∆∞·ªõc ngo√†i' in combined_text or 'kh√¥ng ƒë∆∞·ª£c' in combined_text:
                reasoning_parts.append("- ‚ö†Ô∏è **H·∫°n ch·∫ø**: Ng∆∞·ªùi n∆∞·ªõc ngo√†i kh√¥ng ƒë∆∞·ª£c s·ªü h·ªØu ƒë·∫•t n√¥ng nghi·ªáp t·∫°i Vi·ªát Nam")
            if 'di·ªán t√≠ch' in combined_text:
                reasoning_parts.append("- ‚úì **C√≥ gi·ªõi h·∫°n**: C√≥ quy ƒë·ªãnh v·ªÅ di·ªán t√≠ch s·ªü h·ªØu t·ªëi ƒëa")
        
        if 'quy·ªÅn s·ª≠ d·ª•ng' in combined_text:
            reasoning_parts.append("- ‚úì **C√≥ th·ªÉ**: B·∫°n c√≥ th·ªÉ c√≥ quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, tu·ª≥ theo lo·∫°i ƒë·∫•t")
    
    elif action == 'b√°n':
        reasoning_parts.append("**V·ªÅ vi·ªác b√°n/chuy·ªÉn nh∆∞·ª£ng:**")
        if 'th·ªß t·ª•c' in combined_text:
            reasoning_parts.append("- üìù **Y√™u c·∫ßu**: Ph·∫£i th·ª±c hi·ªán ƒë·∫ßy ƒë·ªß th·ªß t·ª•c ph√°p l√Ω")
        if 'c·∫•p gi·∫•y' in combined_text or 'ch·ª©ng ch·ªâ' in combined_text:
            reasoning_parts.append("- ‚úì **C·∫ßn**: Ph·∫£i c√≥ gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t")
    
    elif action == 'x√¢y_d·ª±ng':
        reasoning_parts.append("**V·ªÅ vi·ªác x√¢y d·ª±ng:**")
        if 'gi·∫•y ph√©p x√¢y d·ª±ng' in combined_text:
            reasoning_parts.append("- ‚ö†Ô∏è **B·∫Øt bu·ªôc**: Ph·∫£i c√≥ gi·∫•y ph√©p x√¢y d·ª±ng")
        if 'quy ho·∫°ch' in combined_text:
            reasoning_parts.append("- ‚úì **Tu√¢n th·ªß**: Ph·∫£i tu√¢n theo quy ho·∫°ch s·ª≠ d·ª•ng ƒë·∫•t")
    
    elif action == 'cho_thu√™':
        reasoning_parts.append("**V·ªÅ vi·ªác cho thu√™:**")
        if 'h·ª£p ƒë·ªìng' in combined_text:
            reasoning_parts.append("- üìã **C·∫ßn**: Ph·∫£i l·∫≠p h·ª£p ƒë·ªìng cho thu√™ r√µ r√†ng")
        if 'th·ªùi h·∫°n' in combined_text:
            reasoning_parts.append("- ‚è∞ **L∆∞u √Ω**: Ph·∫£i x√°c ƒë·ªãnh r√µ th·ªùi h·∫°n cho thu√™")
    
    # Add conditions analysis
    if context.get('conditions'):
        reasoning_parts.append("\n**C√°c ƒëi·ªÅu ki·ªán √°p d·ª•ng:**")
        for cond in context['conditions']:
            reasoning_parts.append(f"- {cond}")
    
    return "\n".join(reasoning_parts)


def extract_numbers_from_text(text: str) -> Dict:
    """Extract numerical information (fees, penalties, time limits) from text."""
    numbers_info = {
        'penalties': [],  # ph·∫°t ti·ªÅn
        'fees': [],  # l·ªá ph√≠
        'time_limits': [],  # th·ªùi h·∫°n
        'percentages': [],  # t·ª∑ l·ªá, %
        'areas': [],  # di·ªán t√≠ch
    }
    
    # Extract penalties
    penalty_pattern = r'(ph·∫°t ti·ªÅn|m·ª©c ph·∫°t|l·ªá ph√≠)[\s:]*(\d+[\.,]?\d*)\s*(tri·ªáu|ngh√¨n|ƒë·ªìng|%|nƒÉm)'
    penalties = re.findall(penalty_pattern, text.lower())
    if penalties:
        numbers_info['penalties'] = [f"{p[1]} {p[2]}" for p in penalties]
    
    # Extract time limits
    time_pattern = r'(th·ªùi h·∫°n|t·ªëi ƒëa|t·ªëi thi·ªÉu)[\s:]*(\d+)\s*(nƒÉm|th√°ng|ng√†y|bu·ªïi)'
    times = re.findall(time_pattern, text.lower())
    if times:
        numbers_info['time_limits'] = [f"{t[1]} {t[2]}" for t in times]
    
    # Extract percentages
    percent_pattern = r'(\d+[\.,]?\d*)\s*%'
    percentages = re.findall(percent_pattern, text)
    if percentages:
        numbers_info['percentages'] = percentages
    
    # Extract area/land measurements
    area_pattern = r'(di·ªán t√≠ch|m¬≤)[\s:]*(\d+[\.,]?\d*)'
    areas = re.findall(area_pattern, text.lower())
    if areas:
        numbers_info['areas'] = [a[1] for a in areas]
    
    return numbers_info


def generate_comparison_analysis(query: str, hits: List[Dict]) -> str:
    """Generate comparison and differentiation analysis for complex scenarios."""
    if len(hits) < 2:
        return ""
    
    comparison_parts = []
    comparison_parts.append("### üîç So s√°nh v√† ƒë·ªëi chi·∫øu:\n")
    
    # Extract key info from multiple sources
    comparison_parts.append("**Theo c√°c quy ƒë·ªãnh kh√°c nhau:**")
    
    for idx, hit in enumerate(hits[:3], 1):
        title = hit.get('title') or f"Quy ƒë·ªãnh {idx}"
        noi_dung = hit.get('noi_dung', '')
        
        # Get first 200 chars of content
        if isinstance(noi_dung, list) and noi_dung:
            if isinstance(noi_dung[0], dict):
                content = noi_dung[0].get('noi_dung', '')[:150]
            else:
                content = str(noi_dung[0])[:150]
        else:
            content = str(noi_dung)[:150]
        
        comparison_parts.append(f"\n**{idx}. {title}:**\n- {content}...")
    
    return "\n".join(comparison_parts)


def generate_practical_advice(query: str, context: Dict, scenario_hits: List[Dict]) -> str:
    """Generate practical advice and recommendations for real-world scenarios."""
    advice_parts = []
    advice_parts.append("### üí° L·ªùi khuy√™n th·ª±c t·∫ø:\n")
    
    action = context.get('action')
    conditions = context.get('conditions')
    
    # General recommendations
    recommendations = []
    
    if action in ('mua', 's·ªü h·ªØu'):
        recommendations = [
            "‚úì ƒê·∫£m b·∫£o b·∫°n hi·ªÉu r√µ lo·∫°i ƒë·∫•t v√† quy·ªÅn s·ª≠ d·ª•ng",
            "‚úì Ki·ªÉm tra ƒë·∫ßy ƒë·ªß h·ªì s∆° ph√°p l√Ω v√† gi·∫•y t·ªù li√™n quan",
            "‚úì T∆∞ v·∫•n v·ªõi c∆° quan ƒë·∫•t ƒëai ƒë·ªãa ph∆∞∆°ng tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh",
            "‚úì L·∫≠p h·ª£p ƒë·ªìng mua b√°n r√µ r√†ng, c√≥ ch·ª©ng th·ª±c",
        ]
    elif action == 'b√°n':
        recommendations = [
            "‚úì Chu·∫©n b·ªã ƒë·∫ßy ƒë·ªß gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng",
            "‚úì Th·ª±c hi·ªán ƒë√∫ng th·ªß t·ª•c c√¥ng khai/h·∫°n ch·∫ø (n·∫øu c√≥)",
            "‚úì L·∫≠p h·ª£p ƒë·ªìng b√°n r√µ r√†ng, c√≥ gi√°c th∆∞∆°ng",
            "‚úì Ho√†n th√†nh th·ªß t·ª•c chuy·ªÉn quy·ªÅn t·∫°i c∆° quan",
        ]
    elif action == 'x√¢y_d·ª±ng':
        recommendations = [
            "‚úì Xin c·∫•p gi·∫•y ph√©p x√¢y d·ª±ng t·ª´ ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng",
            "‚úì Tu√¢n th·ªß quy ho·∫°ch chung c·ªßa khu v·ª±c",
            "‚úì Chu·∫©n b·ªã b·∫£n v·∫Ω ki·∫øn tr√∫c ph√π h·ª£p",
            "‚úì Ki·ªÉm tra c√°c quy ƒë·ªãnh v·ªÅ m·∫≠t ƒë·ªô x√¢y d·ª±ng",
        ]
    elif action == 'cho_thu√™':
        recommendations = [
            "‚úì L·∫≠p h·ª£p ƒë·ªìng cho thu√™ c√≥ x√°c th·ª±c",
            "‚úì Th·ªèa thu·∫≠n r√µ ti·ªÅn thu√™, th·ªùi h·∫°n, b·∫£o h√†nh",
            "‚úì Ghi r√µ c√°c quy·ªÅn v√† nghƒ©a v·ª• c·ªßa hai b√™n",
            "‚úì Ki·ªÉm tra ph√°p l√Ω tr∆∞·ªõc khi k√Ω k·∫øt",
        ]
    else:
        recommendations = [
            "‚úì T√¨m hi·ªÉu k·ªπ c√°c quy ƒë·ªãnh li√™n quan",
            "‚úì T∆∞ v·∫•n chuy√™n gia ph√°p l√Ω khi c·∫ßn",
            "‚úì Chu·∫©n b·ªã h·ªì s∆° ƒë·∫ßy ƒë·ªß v√† r√µ r√†ng",
            "‚úì Tu√¢n th·ªß quy tr√¨nh h√†nh ch√≠nh",
        ]
    
    advice_parts.append("**C√°c b∆∞·ªõc ƒë·ªÅ xu·∫•t:**")
    for rec in recommendations:
        advice_parts.append(rec)
    
    # Add warning if applicable
    if context.get('requires_business_permit'):
        advice_parts.append("\n‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**")
        advice_parts.append("- N·∫øu m·ª•c ƒë√≠ch kinh doanh/c√≥ l·ª£i nhu·∫≠n, c√≥ th·ªÉ √°p d·ª•ng th√™m quy ƒë·ªãnh kh√°c")
        advice_parts.append("- H√£y x√°c nh·∫≠n v·ªõi c∆° quan thu·∫ø v√† qu·∫£n l√Ω kinh doanh ƒë·ªãa ph∆∞∆°ng")
    
    return "\n".join(advice_parts)


def extract_key_phrases(text: str) -> List[str]:
    """Extract key phrases from query for better context understanding."""
    # Remove common Vietnamese stop words
    stop_words = {'c√°c', 'v√†', 'hay', 'hay l√†', 'c√≥', 'l√†', 'ƒë∆∞·ª£c', 'ƒë·ªÉ', 'trong', '·ªü', 'v·ªÅ', 't·ª´', 'v·ªõi', 'nh∆∞', 'c√°i'}
    words = text.lower().split()
    phrases = [w for w in words if w not in stop_words and len(w) > 2]
    return phrases[:5]  # Return top 5 key phrases


def verify_answer_relevance(query: str, answer: str, hits: List[Dict]) -> bool:
    """Verify if the answer is actually relevant to the query.
    
    Returns False if answer seems unrelated (e.g., doesn't contain key terms from hits).
    """
    query_lower = query.lower()
    answer_lower = answer.lower()
    
    # Extract key terms from query
    query_terms = [w for w in query_lower.split() if len(w) > 3]
    
    # Check if at least some key terms appear in answer
    matching_terms = sum(1 for term in query_terms if term in answer_lower)
    
    # If less than 30% of key terms are in answer, it might be irrelevant
    relevance_ratio = matching_terms / max(1, len(query_terms))
    
    # Also check if first hit's title/section appears in answer (as source verification)
    if hits:
        first_hit_info = (hits[0].get('title', '') + ' ' + hits[0].get('section', '')).lower()
        # If hit info has substantial overlap with answer, it's likely relevant
        if any(word in answer_lower for word in first_hit_info.split() if len(word) > 4):
            return True
    
    return relevance_ratio > 0.25  # At least 25% of query terms should match


def summarize_snippet(text: str, max_length: int = 500) -> str:
    """Intelligently summarize a snippet by keeping key sentences."""
    sentences = re.split(r'[.!?]\s+', text)
    result = []
    current_length = 0
    
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        # Prioritize sentences with important keywords
        importance_score = sum(1 for kw in ['quy·ªÅn', 'nghƒ©a v·ª•', 'ƒëi·ªÅu ki·ªán', 'vi ph·∫°m', 'ph·∫°t'] if kw in sent.lower())
        
        if current_length + len(sent) <= max_length:
            result.append(sent)
            current_length += len(sent) + 1
        elif importance_score > 0 and current_length < max_length:
            result.append(sent)
            current_length += len(sent) + 1
            break
    
    return '. '.join(result) + '.' if result else text[:max_length]


def check_definition_exists_in_db(query_term: str) -> Tuple[bool, str]:
    """Check if a definition for the query term actually exists in Article 3 of tinydb.json.
    
    Returns (exists: bool, definition: str)
    """
    try:
        from tinydb import TinyDB
        db = TinyDB("data/tinydb.json", encoding='utf-8')
        
        # Find Article 3 (ƒêi·ªÅu 3)
        articles = db.all()
        article_3 = None
        for article in articles:
            section = article.get('section', '')
            if 'ƒêi·ªÅu 3' in section:
                article_3 = article
                break
        
        if not article_3:
            return False, ""
        
        # Get the definitions content
        noi_dung = article_3.get('noi_dung', '')
        if isinstance(noi_dung, list):
            noi_dung = ' '.join(str(item) for item in noi_dung)
        
        # Clean query term
        query_term_lower = query_term.lower().strip()
        # Remove "l√† g√¨?" suffix if present
        query_term_lower = re.sub(r'\s*(l√† g√¨|l√†|ƒë∆∞·ª£c hi·ªÉu l√†|c√≥ nghƒ©a l√†)\?*$', '', query_term_lower)
        
        # Search for definition pattern: "term l√† ..." or "term:"
        # Definitions in Article 3 are typically numbered: "1. term l√† ...", "2. term l√† ..."
        
        # Split by periods to find definition lines
        definition_lines = re.split(r'(?<=[.;])\s*', noi_dung)
        
        for line in definition_lines:
            line_lower = line.lower()
            # Look for pattern where term appears followed by "l√†" (means)
            if query_term_lower in line_lower:
                # Check if this line contains a definition marker
                if re.search(rf'\b{re.escape(query_term_lower)}\s*(l√†|:|\s*-)', line_lower):
                    # Extract the definition
                    match = re.search(rf'({re.escape(query_term_lower)}\s*(?:l√†|:|-)?[^.;]*[.;]?)', line, re.IGNORECASE)
                    if match:
                        definition = match.group(1).strip()
                        return True, definition
        
        return False, ""
    
    except Exception as e:
        print(f"Error checking definition: {e}")
        return False, ""


def calculate_confidence(scores: List[float], query: str, hits: List[Dict]) -> Tuple[str, float]:
    """Calculate confidence level based on retrieval scores and query-result alignment.
    
    More conservative scoring to avoid false confidence.
    """
    if not scores:
        return 'low', 0.0
    
    avg_score = sum(scores[:3]) / max(1, len(scores[:3]))
    
    # Check for exact matches (ƒêi·ªÅu X)
    article_match = re.search(r'ƒëi·ªÅu\s+(\d+)', query.lower())
    if article_match:
        for h in hits:
            section = (h.get('section') or '') + ' ' + (h.get('title') or '')
            if f"ƒëi·ªÅu {article_match.group(1)}" in section.lower():
                return 'very_high', min(0.99, avg_score + 0.2)
    
    # More conservative thresholds to avoid false confidence
    # Only 'very_high' for very strong matches (0.85+)
    if avg_score >= 0.85:
        return 'very_high', avg_score
    elif avg_score >= 0.65:
        return 'high', avg_score
    elif avg_score >= 0.45:
        return 'medium', avg_score
    else:
        return 'low', avg_score


def compose_answer(intent: str, hits: List[Dict], query: str, confidence_level: str, is_scenario: bool = False, scenario_context: Optional[Dict] = None) -> Tuple[str, str]:
    """Dynamically compose answer based on intent and hits (AI-like generation).
    
    For scenarios: includes reasoning, comparison, and practical advice.
    
    Returns: (answer: str, updated_confidence_level: str)
    """
    if not hits:
        return random.choice(NO_RESULT_TEMPLATES), confidence_level
    
    scores = [h.get('score', 0) for h in hits]
    updated_confidence_level = confidence_level
    
    # Build context-aware intro
    intro = CONFIDENCE_PREFIXES.get(confidence_level, "M√¨nh t√¨m ƒë∆∞·ª£c th√¥ng tin sau:")
    
    # If this is a scenario query, build comprehensive response
    if is_scenario and scenario_context:
        response_parts = []
        response_parts.append(intro)
        response_parts.append("")
        
        # Add scenario analysis
        scenario_analysis = analyze_scenario(query, scenario_context, hits)
        if scenario_analysis:
            response_parts.append(scenario_analysis)
            response_parts.append("")
        
        # Add numerical/regulatory info
        numbers_info = {}
        for h in hits:
            noi_dung = h.get('noi_dung', '')
            text_to_search = str(noi_dung)
            extracted = extract_numbers_from_text(text_to_search)
            for key in extracted:
                if extracted[key]:
                    numbers_info[key] = extracted[key]
        
        if numbers_info:
            response_parts.append("### üìä Th√¥ng tin s·ªë li·ªáu:")
            if numbers_info.get('penalties'):
                response_parts.append(f"- M·ª©c ph·∫°t: {', '.join(numbers_info['penalties'])}")
            if numbers_info.get('time_limits'):
                response_parts.append(f"- Th·ªùi h·∫°n: {', '.join(numbers_info['time_limits'])}")
            if numbers_info.get('percentages'):
                response_parts.append(f"- T·ª∑ l·ªá: {', '.join(numbers_info['percentages'])}%")
            response_parts.append("")
        
        # Add comparison
        comparison = generate_comparison_analysis(query, hits)
        if comparison:
            response_parts.append(comparison)
            response_parts.append("")
        
        # Add practical advice
        advice = generate_practical_advice(query, scenario_context, hits)
        if advice:
            response_parts.append(advice)
            response_parts.append("")
        
        response_parts.append(CONFIDENCE_SUFFIXES.get(confidence_level, ""))
        return "\n".join(response_parts), updated_confidence_level
    
    # Original logic for non-scenario queries
    
    scores = [h.get('score', 0) for h in hits]
    
    # Build context-aware intro
    intro = CONFIDENCE_PREFIXES.get(confidence_level, "M√¨nh t√¨m ƒë∆∞·ª£c th√¥ng tin sau:")
    
    # Helper: safely extract text (generic, for non-article intents)
    def get_text(h):
        """Extract text from hit"""
        text = h.get('text') or h.get('noi_dung') or ''
        
        # Convert list items to strings
        if isinstance(text, list):
            text_parts = []
            for item in text:
                if isinstance(item, str):
                    text_parts.append(item)
                elif isinstance(item, dict):
                    dict_text = item.get('noi_dung') or item.get('text') or item.get('content') or str(item)
                    text_parts.append(str(dict_text)[:200])  # Limit each dict to 200 chars
                else:
                    text_parts.append(str(item)[:200])
            text = ' '.join(text_parts)
        return str(text)
    
    # For article intent - direct quote with context
    if intent == 'article':
        article_match = re.search(r'ƒëi·ªÅu\s+(\d+)', query.lower())
        if article_match:
            article_num = article_match.group(1)
            for h in hits:
                noi_dung = h.get('noi_dung', '')
                # If noi_dung is a list of article dicts, find the matching one
                if isinstance(noi_dung, list):
                    for article in noi_dung:
                        if isinstance(article, dict) and str(article.get('dieu_so', '')) == str(article_num):
                            text = article.get('noi_dung', '')
                            if text:
                                return f"{intro}\n\n**ƒêi·ªÅu {article_num}:**\n\n{text}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
        
        # Fallback: use top hit, but take first article only
        top_hit = hits[0]
        noi_dung = top_hit.get('noi_dung', '')
        if isinstance(noi_dung, list) and noi_dung:
            text = noi_dung[0].get('noi_dung', str(noi_dung[0])) if isinstance(noi_dung[0], dict) else str(noi_dung[0])
        else:
            text = str(noi_dung or '')
        return f"{intro}\n\n{text[:1000]}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
    
    # For definition intent - extract and explain  
    if intent == 'definition':
        # Extract the term from query (remove "l√† g√¨?" suffix)
        query_term = re.sub(r'\s*(l√† g√¨|l√†)\?*$', '', query.lower()).strip()
        
        # For well-known legal terms, provide concise official definition
        known_definitions = {
            'quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t': 'Quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t l√† quy·ªÅn c·ªßa ng∆∞·ªùi ƒë∆∞·ª£c Nh√† n∆∞·ªõc giao ƒë·∫•t, cho thu√™ ƒë·∫•t, c√¥ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t ƒë·ªÉ khai th√°c, s·ª≠ d·ª•ng ƒë·∫•t theo quy ƒë·ªãnh c·ªßa Lu·∫≠t.',
            'ƒë·∫•t ƒëai': 'ƒê·∫•t ƒëai l√† to√†n b·ªô l√£nh th·ªï ƒë·∫•t li·ªÅn l·∫°c v√† ƒë·∫£o c·ªßa Vi·ªát Nam, bao g·ªìm m·∫∑t ƒë·∫•t, l√≤ng ƒë·∫•t, t√†i nguy√™n tr√™n b·ªÅ m·∫∑t ƒë·∫•t.',
            'ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t': 'Ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t l√† ng∆∞·ªùi ƒë∆∞·ª£c Nh√† n∆∞·ªõc giao ƒë·∫•t, cho thu√™ ƒë·∫•t, c√¥ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t ho·∫∑c nh·∫≠n chuy·ªÉn quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t theo quy ƒë·ªãnh c·ªßa Lu·∫≠t.',
        }
        
        # Check if we have a known definition
        if query_term in known_definitions:
            definition = known_definitions[query_term]
            # Use high confidence for known definitions
            return f"D·ª±a tr√™n c√°c t√†i li·ªáu ph√°p lu·∫≠t:\n\n{definition}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", confidence_level
        
        # Otherwise, look for definition in hits
        found_def = False
        for h in hits:
            text = get_text(h)
            sentences = re.split(r'[.!?]\s+', text)
            
            for sent in sentences[:15]:
                sent = sent.strip()
                if not sent or len(sent) < 20:
                    continue
                    
                sent_lower = sent.lower()
                # Look for definition pattern: "query_term l√† ..."
                definition_pattern = re.escape(query_term) + r'\s+l√†'
                if re.search(definition_pattern, sent_lower):
                    if verify_answer_relevance(query, sent, hits):
                        # Extract just the definition sentence
                        found_def = True
                        return f"D·ª±a tr√™n c√°c t√†i li·ªáu ph√°p lu·∫≠t:\n\n{sent.strip()}.\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", confidence_level
        
        # If no definition found, provide related info with medium confidence
        if not found_def:
            updated_confidence_level = 'medium'
            top_hit = hits[0]
            text = get_text(top_hit)
            # Only take first 1-2 sentences for conciseness
            first_sentences = re.split(r'[.!?]', text)[:2]
            text = '. '.join([s.strip() for s in first_sentences if s.strip()]) + '.'
            return f"Th√¥ng tin li√™n quan:\n\n{text}\n\n{CONFIDENCE_SUFFIXES.get(updated_confidence_level, '')}", updated_confidence_level
        
        return f"{intro}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", confidence_level
    
    # For procedure intent - list steps clearly
    if intent == 'procedure':
        steps = []
        for h in hits:
            text = get_text(h)
            sentences = re.split(r'[.!?;,-]\s+', text)
            
            for sent in sentences:
                sent = sent.strip()
                if sent and any(verb in sent.lower() for verb in ['n·ªôp', 'l·∫≠p', 'xin', 'c·∫•p', 'tr√¨nh', 'ho√†n th√†nh', 'th·ª±c hi·ªán', 'g·ª≠i', 'khai', 'ƒë·ªÅ ngh·ªã']):
                    steps.append(sent)
                if len(steps) >= 4:
                    break
            if steps:
                break
        
        if steps:
            step_text = '\n'.join([f"{i+1}. {s}" for i, s in enumerate(steps[:5])])
            return f"{intro}\n\n{step_text}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
    
    # For penalty/violation intent
    if intent == 'penalty':
        top_hit = hits[0]
        text = get_text(top_hit)
        
        # Extract penalty-related sentences
        sentences = re.split(r'[.!?]\s+', text)
        penalty_sents = [s for s in sentences if any(kw in s.lower() for kw in ['ph·∫°t', 'x·ª≠ ph·∫°t', 'm·ª©c ph·∫°t', 'ti·ªÅn ph·∫°t', 'h√†nh ch√≠nh'])]
        
        if penalty_sents:
            penalty_text = '. '.join(penalty_sents[:3]) + '.'
            return f"{intro}\n\n{penalty_text}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
        
        text = summarize_snippet(text, 400)
        return f"{intro}\n\n{text}\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
    
    # For time/duration/limit intent - extract the most relevant time information
    if intent == 'time_limit':
        # Look through hits for time-related information
        for h in hits:
            text = get_text(h)
            sentences = re.split(r'[.!?]\s+', text)
            
            # Find sentences with time keywords
            for sent in sentences:
                sent = sent.strip()
                if any(kw in sent.lower() for kw in ['th·ªùi h·∫°n', 'nƒÉm', 'th√°ng', 'ng√†y', 't·ªëi ƒëa', 't·ªëi thi·ªÉu']):
                    if len(sent) > 20:  # Meaningful sentence
                        # Verify relevance
                        if verify_answer_relevance(query, sent, hits):
                            return f"{intro}\n\n{sent}.\n\n{CONFIDENCE_SUFFIXES.get(confidence_level, '')}", updated_confidence_level
        
        # Fallback: summarize top hit
        top_hit = hits[0]
        text = get_text(top_hit)
        # If not relevant, downgrade confidence
        if not verify_answer_relevance(query, text, hits):
            updated_confidence_level = 'low'
            intro = CONFIDENCE_PREFIXES.get(updated_confidence_level, "M√¨nh t√¨m ƒë∆∞·ª£c th√¥ng tin sau:")
        text = summarize_snippet(text, 400)
        return f"{intro}\n\n{text}\n\n{CONFIDENCE_SUFFIXES.get(updated_confidence_level, '')}", updated_confidence_level
    
    # General/WHO intent - focus on BEST result only (not all 3)
    top_hit = hits[0]
    text = get_text(top_hit)
    
    # Verify answer relevance
    is_relevant = verify_answer_relevance(query, text, hits)
    
    # If relevance is low, downgrade confidence
    if not is_relevant and confidence_level in ['very_high', 'high']:
        updated_confidence_level = 'low'
        intro = CONFIDENCE_PREFIXES.get(updated_confidence_level, "M√¨nh t√¨m ƒë∆∞·ª£c th√¥ng tin sau:")
    
    # If hit has high score, use it directly with longer summary
    if top_hit.get('score', 0) > 0.6:
        summary = summarize_snippet(text, 500)
        return f"{intro}\n\n{summary}\n\n{CONFIDENCE_SUFFIXES.get(updated_confidence_level, '')}", updated_confidence_level
    
    # If lower score, try showing top 2 results only (not 3)
    summaries = []
    for i, h in enumerate(hits[:2], 1):
        text = get_text(h)
        summary = summarize_snippet(text, 250)
        title = h.get('title') or 'Th√¥ng tin'
        summaries.append(f"**{i}. {title}:**\n{summary}")
    
    combined = '\n\n'.join(summaries)
    return f"{intro}\n\n{combined}\n\n{CONFIDENCE_SUFFIXES.get(updated_confidence_level, '')}", updated_confidence_level



def detect_intent(q: str) -> str:
    """Detect user intent from query with multi-level matching."""
    ql = (q or '').lower()
    
    # Check for greetings
    if any(w in ql for w in ['xin ch√†o', 'ch√†o', 'hello', 'hi', 'halo', 'bay', 'h·∫ø l√¥']):
        return 'greeting'
    
    # Article-specific queries
    if re.search(r"\bƒëi[e√™]u\b|\bdieu\b", ql):
        return 'article'
    
    # Definition queries
    if any(w in ql for w in ['l√† g√¨', 'ƒë·ªãnh nghƒ©a', 'ƒë∆∞·ª£c hi·ªÉu', 'ƒë∆∞·ª£c g·ªçi', 'c√≥ nghƒ©a', 't·ª©c l√†', 'kh√°i ni·ªám', '√Ω nghƒ©a']):
        return 'definition'
    
    # Time/Duration/Limit queries
    if any(w in ql for w in ['bao l√¢u', 'th·ªùi h·∫°n', 'khi n√†o', 't·ªëi ƒëa', 't·ªëi thi·ªÉu', 'bao gi·ªù', 'm·∫•y nƒÉm', 'm·∫•y th√°ng', 'm·∫•y ng√†y']):
        return 'time_limit'
    
    # Procedure/process queries
    if any(w in ql for w in ['th·ªß t·ª•c', 'h·ªì s∆°', 'n·ªôp', 'xin', 'c√°ch th·ª©c', 'l√†m sao', 'c√°ch n√†o', 'b∆∞·ªõc', 'quy tr√¨nh', 'process']):
        return 'procedure'
    
    # Penalty/violation queries
    if any(w in ql for w in ['ph·∫°t', 'x·ª≠ ph·∫°t', 'm·ª©c ph·∫°t', 'vi ph·∫°m', 'h√¨nh ph·∫°t', 'x·ª≠ l√Ω', 'h·∫≠u qu·∫£']):
        return 'penalty'
    
    # WHO/actor queries
    if any(w in ql for w in ['ai', 'ng∆∞·ªùi', 'c∆° quan', 'ch·ªß th·ªÉ', 'c√≥ quy·ªÅn', 'ph·∫£i', 't·ªï ch·ª©c', 'doanh nghi·ªáp']):
        return 'who'
    
    return 'general'


def answer_question(query: str, k: int = 5, session_id: str = None, user_id: str = "anonymous") -> Dict:
    """Advanced answer generation with reasoning, learning, and sentiment analysis.
    
    Features:
    - Smart intent detection
    - Context-aware answer composition
    - Confidence scoring
    - Multi-source synthesis
    - Scenario reasoning & practical advice
    - Numerical analysis (penalties, time limits, fees)
    - Learning from feedback
    - Sentiment analysis for tone adjustment
    - Natural language generation for varied responses
    """
    q = (query or "").strip()
    
    # Get engines
    learning_engine = get_learning_engine()
    sentiment_analyzer = get_sentiment_analyzer()
    conversation_manager = get_conversation_manager()
    nlg_engine = get_nlg_engine()
    
    # Handle empty query
    if not q:
        return {"answer": "B·∫°n h√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n. T√¥i s·∫µn s√†ng gi√∫p! üòä", "sources": []}

    # ============ SENTIMENT & CONTEXT ANALYSIS ============
    sentiment, sentiment_conf = sentiment_analyzer.analyze_sentiment(q)
    urgency, urgency_conf = sentiment_analyzer.analyze_urgency(q)
    is_followup = sentiment_analyzer.is_follow_up_question(q)
    context_type = sentiment_analyzer.detect_context_type(q)
    
    # Get response tone based on sentiment & urgency
    response_tone = sentiment_analyzer.get_response_tone(sentiment, urgency)
    
    # ============ CONTEXT MEMORY ============
    context_window = {}
    if session_id:
        context_window = conversation_manager.get_context_window(session_id)
    
    # ============ SCENARIO & INTENT DETECTION ============
    # Detect if this is a scenario query (practical situation)
    is_scenario = detect_scenario_query(q)
    scenario_context = None
    
    if is_scenario:
        scenario_context = extract_scenario_context(q)
        intent = 'scenario'
    else:
        intent = detect_intent(q)
    
    # Handle greetings
    if intent == 'greeting':
        answer = random.choice(GREETING_RESPONSES)
        if session_id:
            conversation_manager.add_message(session_id, "user", q)
            conversation_manager.add_message(session_id, "bot", answer)
        return {"answer": answer, "sources": [], "sentiment": sentiment.value}

    # ============ CHECK LEARNED PATTERNS ============
    # T√¨m c√¢u tr·∫£ l·ªùi t∆∞∆°ng t·ª± t·ª´ d·ªØ li·ªáu ƒë√£ h·ªçc
    # BUT: Skip learning for definition queries to avoid returning unverified definitions
    learned_similar = []
    if intent != 'definition':
        learned_similar = learning_engine.find_similar_learned_answers(q, top_k=2)
    
    if learned_similar and learned_similar[0]["similarity"] > 0.7:
        # S·ª≠ d·ª•ng c√¢u tr·∫£ l·ªùi ƒë√£ h·ªçc n·∫øu ƒë·ªô t∆∞∆°ng t·ª± cao
        learned_answer = learned_similar[0]["answer"]
        # Paraphrase ƒë·ªÉ tr√°nh l·∫∑p l·∫°i t·ª´ng ch·ªØ
        answer = nlg_engine.paraphrase(learned_answer, style="informal")
        
        interaction_id = learning_engine.record_interaction(
            q, answer, [], user_id, 
            {"from_learning": True, "similarity": learned_similar[0]["similarity"]}
        )
        
        if session_id:
            conversation_manager.add_message(session_id, "user", q, {"sentiment": sentiment.value})
            conversation_manager.add_message(session_id, "bot", answer, {"interaction_id": interaction_id})
        
        return {
            "answer": answer,
            "sources": [],
            "confidence": learned_similar[0]["similarity"],
            "sentiment": sentiment.value,
            "from_learning": True,
            "interaction_id": interaction_id
        }

    # Choose retrieval mode based on intent
    mode = None
    if intent == 'article':
        mode = 'article'
    elif intent in ('definition', 'who', 'procedure', 'penalty'):
        mode = 'keyword'

    # Retrieve relevant documents
    hits = retrieve(q, k=k, mode=mode)
    if not hits:
        no_result_answer = random.choice(NO_RESULT_TEMPLATES)
        
        if session_id:
            conversation_manager.add_message(session_id, "user", q, {"sentiment": sentiment.value})
            conversation_manager.add_message(session_id, "bot", no_result_answer)
        
        return {
            "answer": no_result_answer,
            "sources": [],
            "sentiment": sentiment.value
        }

    # Calculate confidence
    scores = [h.get('score', 0) for h in hits]
    confidence_level, conf_score = calculate_confidence(scores, q, hits)
    
    # Collect sources
    sources = []
    for h in hits[:3]:
        url = h.get('url') or h.get('nguon')
        if url:
            sources.append(url)
    
    # ============ GENERATE ANSWER ============
    # Generate answer using AI-like composition with scenario analysis
    answer, updated_confidence_level = compose_answer(
        intent, hits, q, confidence_level,
        is_scenario=is_scenario,
        scenario_context=scenario_context
    )
    
    # Update confidence if it was downgraded during composition
    if updated_confidence_level != confidence_level:
        confidence_level = updated_confidence_level
        # Recalculate conf_score based on new level
        if confidence_level == 'very_high':
            conf_score = 0.95
        elif confidence_level == 'high':
            conf_score = 0.75
        elif confidence_level == 'medium':
            conf_score = 0.55
        else:  # low
            conf_score = 0.35
    
    # ============ APPLY TONE BASED ON SENTIMENT ============
    # Th√™m tone prefix n·∫øu c·∫ßn
    if response_tone.get("greeting"):
        answer = f"{response_tone['greeting']}\n\n{answer}"
    
    # Th√™m suffix
    if response_tone.get("suffix"):
        answer = f"{answer}\n\n{response_tone['suffix']}"
    
    # ============ ADD EMOJI & FORMATTING ============
    answer = nlg_engine.add_emojis(answer)
    
    # ============ RECORD INTERACTION ============
    interaction_id = learning_engine.record_interaction(
        q, answer, sources, user_id,
        {
            "sentiment": sentiment.value,
            "urgency": urgency.value,
            "intent": intent,
            "confidence": conf_score,
            "context_type": context_type
        }
    )
    
    # ============ ADD TO CONVERSATION ============
    if session_id:
        conversation_manager.add_message(session_id, "user", q, {"sentiment": sentiment.value, "urgency": urgency.value})
        conversation_manager.add_message(session_id, "bot", answer, {"interaction_id": interaction_id})
    
    return {
        "answer": answer,
        "sources": list(dict.fromkeys(sources)),
        "confidence": conf_score,
        "is_scenario": is_scenario,
        "sentiment": sentiment.value,
        "urgency": urgency.value,
        "interaction_id": interaction_id,
        "is_followup": is_followup
    }



